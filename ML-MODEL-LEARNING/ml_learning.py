# -*- coding: utf-8 -*-
"""ML learning

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GHW4fuvp6bSZPb5kLOICjRf9Hu4fqbIB
"""

!pip install -q sklearn

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x

from __future__ import absolute_import, division, print_function, unicode_literals

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import clear_output
from six.moves import urllib

import tensorflow.compat.v2.feature_column as fc
import tensorflow as tf

dftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv') #training
dfeval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') #testing
dftrain.head() #shows first 5 entries in our dataset
y_train = dftrain.pop('survived') #removes the survived column 
y_eval = dfeval.pop('survived')

dftrain.age.hist(bins = 20)

dftrain.sex.value_counts().plot(kind = 'barh')

dftrain['class'].value_counts().plot(kind = 'barh')

pd.concat([dftrain, y_train], axis=1).groupby('sex').survived.mean().plot(kind="barh").set_xlabel('% survive')

CATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck', 'embark_town', 'alone']
NUMERIC_COLUMNS = ['age', 'fare']

## creating feature columns so we can find all the column names and unique values of each column for our model
feature_columns = []
for feature_name in CATEGORICAL_COLUMNS:
  vocabulary = dftrain[feature_name].unique()
  feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))

for feature_name in NUMERIC_COLUMNS:
  feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype = tf.float32))

print(feature_columns)

##INPUT FUNCTION: How your dataset will be broken into epochs/batches to be fed to your model

def make_input_fn(data_df, label_df, num_epochs = 10, shuffle = True, batch_size = 32): # makinging an input function
  def input_function(): #function in function?
    # we create a tf.data.object type using this function by passing the data from our pandas df and the labels
    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df)) 
    if shuffle:
      ds = ds.shuffle(1000)
     # split the data set into batches which we pass through "batch size" and then repeats the process for the number of epochs
    ds = ds.batch(batch_size).repeat(num_epochs)
    return ds
  return input_function
# making the inputs for both the training data set and the testing/evaluation dataset
# REMINDER: They have both had the survived column removed because that is what we want the model to predict
train_input_fn = make_input_fn(dftrain, y_train) 
# for eval we only need an epoch of one because its used to test how accurate our model is not to train it and don't need to shuffle for the same reason
eval_input_fn = make_input_fn(dfeval, y_eval, num_epochs = 1,shuffle = False)

# CREATION OF THE MODEL

# estimator module provided by tensor flow that we will pass our data through
# feature columns = feature columns??? what's that mean? why cant we just pass in feature columns
# tf.estimator.LinearClassifier documentation definition
"""
Estimator using the default optimizer.
estimator = tf.estimator.LinearClassifier(
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b])
"""

linear_est = tf.estimator.LinearClassifier(feature_columns = feature_columns)

# TRAINING
" REMINDER: train_input_fn is a tensor flow data object type that contains the dataset in dftrain and also the labels in y_train "
linear_est.train(train_input_fn) # Train the model using the inputs batches we created using the input function
result = linear_est.evaluate(eval_input_fn) # Testing the viablility of the model using the batches we created for the evalutation dataset

print(result['accuracy']) # does the evaulate function automatically create an accuracy column